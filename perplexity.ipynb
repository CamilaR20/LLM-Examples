{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Measuring perplexity on fine-tuned versions of LLaMA2\n",
    "\n",
    "Some fine-tuned Llama-2-7b models available through HuggingFace Hub are:\n",
    "\n",
    "1.   [4i-ai/Llama-2-7b-alpaca-es](https://huggingface.co/4i-ai/Llama-2-7b-alpaca-es)\n",
    "2.   [cherrybomb3649/llama-2-7b-imdb](https://huggingface.co/cherrybomb3649/llama-2-7b-imdb)\n",
    "4.   [mrm8488/llama-2-coder-7b](https://huggingface.co/mrm8488/llama-2-coder-7b)\n",
    "5.   [Harshvir/Llama-2-7B-physics](https://huggingface.co/Harshvir/Llama-2-7B-physics)\n",
    "6.   [botch/Llama-2-7b-pubmed](https://huggingface.co/botch/Llama-2-7b-pubmed)\n",
    "7.   [unionai/Llama-2-7b-hf-wikipedia](https://huggingface.co/unionai/Llama-2-7b-hf-wikipedia)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "from transformers import AutoModelForCausalLM, AutoTokenizer, BitsAndBytesConfig\n",
    "import torch\n",
    "from torch.nn import functional as F\n",
    "import accelerate\n",
    "import bitsandbytes  # Works with CUDA\n",
    "import numpy as np\n",
    "from tqdm import tqdm\n",
    "import pandas as pd\n",
    "import time\n",
    "\n",
    "# device = torch.device(\"mps\") if torch.backends.mps.is_built() else torch.device(\"cpu\")  # To run on mac\n",
    "device = \"cuda:0\" if torch.cuda.is_available() else \"cpu\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "8537ff908ea9493ba123910af62fc2ca",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading checkpoint shards:   0%|          | 0/3 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/mce/anaconda3/envs/hf/lib/python3.9/site-packages/transformers/utils/hub.py:374: FutureWarning: The `use_auth_token` argument is deprecated and will be removed in v5 of Transformers. Please use `token` instead.\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "# Get model and tokenizer\n",
    "model_name = \"4i-ai/Llama-2-7b-alpaca-es\"\n",
    "access_token = os.environ[\"HF_API_KEY\"]\n",
    "\n",
    "# Quantization: https://huggingface.co/docs/transformers/v4.33.2/en/main_classes/quantization\n",
    "bnb_config = BitsAndBytesConfig(load_in_4bit=True, bnb_4bit_quant_type=\"nf4\", bnb_4bit_compute_dtype=getattr(torch, \"float16\"), bnb_4bit_use_double_quant=True)\n",
    "\n",
    "model = AutoModelForCausalLM.from_pretrained(model_name, device_map=\"auto\", quantization_config=bnb_config,  token=access_token);  # In colab cache_dir can be set to a folder in GDrive\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_name, use_fast=True, token=access_token);\n",
    "tokenizer.pad_token = tokenizer.eos_token"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Input Sequences and predictions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "prompts_ds = [\"¿Qué significa DNA?\", \"¿Cuál es la capital de Francia?\", \"Encuentra la capital de España.\", \"¿Cuáles son los tres colores primarios?\", \"Genera una lista de 5 adjetivos que describan a una persona como valiente.\"]\n",
    "# prompts_other = [\"Que signifie l'ADN ?\", \"Quelle est la capitale de la France ?\", \"Trouver la capitale de l'Espagne.\", \"Quelles sont les trois couleurs primaires ?\", \"Générer une liste de 5 adjectifs qui décrivent une personne comme courageuse. \"]\n",
    "prompts_other = [\"What is DNA? Answer in english.\", \"What is the capital of France?\", \"What is the capital of Spain?\", \"What are the three primary colors?\", \"Generate a list of 5 adjectives that describe a person as brave.\"]\n",
    "\n",
    "prompts = prompts_ds + prompts_other\n",
    "sources = [\"ds\" for _ in range(len(prompts_ds))] + [\"other\" for _ in range(len(prompts_other))]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Using loss to compute perplexity"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/mce/anaconda3/envs/hf/lib/python3.9/site-packages/transformers/generation/configuration_utils.py:386: UserWarning: `do_sample` is set to `False`. However, `top_p` is set to `0.9` -- this flag is only used in sample-based generation modes. You should set `do_sample=True` or unset `top_p`.\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "# Get predictions from model\n",
    "predictions = []\n",
    "input_predictions = []\n",
    "input_length = []\n",
    "temp = 1\n",
    "\n",
    "for p in prompts:\n",
    "  prompt = \"### Instruction:\\n\"+ p +\"\\n\\n### Response:\\n\"\n",
    "  model_inputs = tokenizer(prompt, return_tensors=\"pt\").to(device)\n",
    "  output = model.generate(**model_inputs, temperature=temp, max_new_tokens=20, do_sample=False, output_scores=True, return_dict_in_generate=True)\n",
    "\n",
    "  predictions.append(tokenizer.decode(output.sequences[0, model_inputs.input_ids.shape[1]:], skip_special_tokens=True))\n",
    "  input_predictions.append(tokenizer.decode(output.sequences[0], skip_special_tokens=True))\n",
    "  input_length.append(model_inputs.input_ids.shape[1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_perplexity(seq, idx):\n",
    "  # Compute perplexity from predictions using model callback\n",
    "  # seq = prompt + prediction, idx = index where the prompt ends and prediction begins\n",
    "  model_inputs = tokenizer(seq, return_tensors=\"pt\").to(device)\n",
    "  input_ids = model_inputs.input_ids.to(device)\n",
    "\n",
    "  target_ids = input_ids.clone()\n",
    "  target_ids[:, :idx] = -100  # Don't compute the loss over the input\n",
    "\n",
    "  with torch.no_grad():\n",
    "    outputs = model(input_ids, labels=target_ids)\n",
    "    neg_log_likelihood = outputs.loss\n",
    "\n",
    "    ppl = torch.exp(neg_log_likelihood)\n",
    "\n",
    "  return ppl\n",
    "\n",
    "perplexities = []\n",
    "for prediction, idx in zip(input_predictions, input_length):\n",
    "  ppl = compute_perplexity(prediction, idx)\n",
    "  perplexities.append(ppl.item())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Prompt</th>\n",
       "      <th>Predictions</th>\n",
       "      <th>Source</th>\n",
       "      <th>Perplexity</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>¿Qué significa DNA?</td>\n",
       "      <td>El ADN es una molécula de biología molecular q...</td>\n",
       "      <td>ds</td>\n",
       "      <td>1.629931</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>¿Cuál es la capital de Francia?</td>\n",
       "      <td>La capital de Francia es París.</td>\n",
       "      <td>ds</td>\n",
       "      <td>1.069187</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Encuentra la capital de España.</td>\n",
       "      <td>La capital de España es Madrid.</td>\n",
       "      <td>ds</td>\n",
       "      <td>1.084588</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>¿Cuáles son los tres colores primarios?</td>\n",
       "      <td>Los tres colores primarios son el rojo, el azu...</td>\n",
       "      <td>ds</td>\n",
       "      <td>1.113913</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Genera una lista de 5 adjetivos que describan ...</td>\n",
       "      <td>Valiente, corajeoso, audaz, heroico, valiente.</td>\n",
       "      <td>ds</td>\n",
       "      <td>1.839290</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>What is DNA? Answer in english.</td>\n",
       "      <td>DNA es una molécula que contiene la informació...</td>\n",
       "      <td>other</td>\n",
       "      <td>1.668605</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>What is the capital of France?</td>\n",
       "      <td>París es la capital de Francia.</td>\n",
       "      <td>other</td>\n",
       "      <td>1.290016</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>What is the capital of Spain?</td>\n",
       "      <td>Madrid es la capital de España.</td>\n",
       "      <td>other</td>\n",
       "      <td>1.177768</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>What are the three primary colors?</td>\n",
       "      <td>Los tres colores primarios son el rojo, el azu...</td>\n",
       "      <td>other</td>\n",
       "      <td>1.153605</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>Generate a list of 5 adjectives that describe ...</td>\n",
       "      <td>Valiente, coraje, audaz, valiente, valiente.</td>\n",
       "      <td>other</td>\n",
       "      <td>1.796446</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                              Prompt  \\\n",
       "0                                ¿Qué significa DNA?   \n",
       "1                    ¿Cuál es la capital de Francia?   \n",
       "2                    Encuentra la capital de España.   \n",
       "3            ¿Cuáles son los tres colores primarios?   \n",
       "4  Genera una lista de 5 adjetivos que describan ...   \n",
       "5                    What is DNA? Answer in english.   \n",
       "6                     What is the capital of France?   \n",
       "7                      What is the capital of Spain?   \n",
       "8                 What are the three primary colors?   \n",
       "9  Generate a list of 5 adjectives that describe ...   \n",
       "\n",
       "                                         Predictions Source  Perplexity  \n",
       "0  El ADN es una molécula de biología molecular q...     ds    1.629931  \n",
       "1                    La capital de Francia es París.     ds    1.069187  \n",
       "2                    La capital de España es Madrid.     ds    1.084588  \n",
       "3  Los tres colores primarios son el rojo, el azu...     ds    1.113913  \n",
       "4     Valiente, corajeoso, audaz, heroico, valiente.     ds    1.839290  \n",
       "5  DNA es una molécula que contiene la informació...  other    1.668605  \n",
       "6                    París es la capital de Francia.  other    1.290016  \n",
       "7                    Madrid es la capital de España.  other    1.177768  \n",
       "8  Los tres colores primarios son el rojo, el azu...  other    1.153605  \n",
       "9       Valiente, coraje, audaz, valiente, valiente.  other    1.796446  "
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "results = pd.DataFrame({\"Prompt\": prompts, \"Predictions\": predictions, \"Source\": sources, \"Perplexity\": perplexities})\n",
    "display(results)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Using PyTorch to compute perplexity\n",
    "[Documentation](https://torchmetrics.readthedocs.io/en/stable/text/perplexity.html#)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/mce/anaconda3/envs/hf/lib/python3.9/site-packages/transformers/generation/configuration_utils.py:386: UserWarning: `do_sample` is set to `False`. However, `top_p` is set to `0.9` -- this flag is only used in sample-based generation modes. You should set `do_sample=True` or unset `top_p`.\n",
      "  warnings.warn(\n",
      "/home/mce/anaconda3/envs/hf/lib/python3.9/site-packages/transformers/generation/utils.py:1591: UserWarning: You are calling .generate() with the `input_ids` being on a device type different than your model's device. `input_ids` is on cpu, whereas the model is on cuda. You may experience unexpected behaviors or slower generation. Please make sure that you have put `input_ids` to the correct device by calling for example input_ids = input_ids.to('cuda') before running `.generate()`.\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "from torchmetrics.text import Perplexity\n",
    "\n",
    "predictions = []\n",
    "perplexities = []\n",
    "temp = 1\n",
    "\n",
    "for i, p in enumerate(prompts):\n",
    "  prompt = \"### Instruction:\\n\"+ p +\"\\n\\n### Response:\\n\"\n",
    "  model_inputs = tokenizer(prompt, return_tensors=\"pt\")\n",
    "\n",
    "  output = model.generate(**model_inputs, temperature=temp, do_sample=False, output_scores=True, return_dict_in_generate=True)\n",
    "\n",
    "  labels = output.sequences[:, model_inputs.input_ids.shape[1]:]\n",
    "  logits = torch.stack(output.scores, dim=1)\n",
    "\n",
    "  perp = Perplexity(ignore_index=-100)\n",
    "  ppl = perp(logits, labels)\n",
    "\n",
    "  predictions.append(tokenizer.decode(labels[0], skip_special_tokens=True))\n",
    "  perplexities.append(ppl.item())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Prompt</th>\n",
       "      <th>Predictions</th>\n",
       "      <th>Source</th>\n",
       "      <th>Perplexity</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>¿Qué significa DNA?</td>\n",
       "      <td>El ADN es una molécula de biología molecular q...</td>\n",
       "      <td>ds</td>\n",
       "      <td>1.704543</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>¿Cuál es la capital de Francia?</td>\n",
       "      <td>La capital de Francia es París.</td>\n",
       "      <td>ds</td>\n",
       "      <td>1.064529</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Encuentra la capital de España.</td>\n",
       "      <td>La capital de España es Madrid.</td>\n",
       "      <td>ds</td>\n",
       "      <td>1.075414</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>¿Cuáles son los tres colores primarios?</td>\n",
       "      <td>Los tres colores primarios son el rojo, el azu...</td>\n",
       "      <td>ds</td>\n",
       "      <td>1.119825</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Genera una lista de 5 adjetivos que describan ...</td>\n",
       "      <td>Valiente, corajeoso, audaz, heroico, valiente.</td>\n",
       "      <td>ds</td>\n",
       "      <td>1.775416</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>What is DNA? Answer in english.</td>\n",
       "      <td>DNA es una molécula que contiene la informació...</td>\n",
       "      <td>other</td>\n",
       "      <td>1.934921</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>What is the capital of France?</td>\n",
       "      <td>París es la capital de Francia.</td>\n",
       "      <td>other</td>\n",
       "      <td>1.256139</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>What is the capital of Spain?</td>\n",
       "      <td>Madrid es la capital de España.</td>\n",
       "      <td>other</td>\n",
       "      <td>1.162318</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>What are the three primary colors?</td>\n",
       "      <td>Los tres colores primarios son el rojo, el azu...</td>\n",
       "      <td>other</td>\n",
       "      <td>1.152615</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>Generate a list of 5 adjectives that describe ...</td>\n",
       "      <td>Valiente, coraje, audaz, valiente, valiente.</td>\n",
       "      <td>other</td>\n",
       "      <td>1.732764</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                              Prompt  \\\n",
       "0                                ¿Qué significa DNA?   \n",
       "1                    ¿Cuál es la capital de Francia?   \n",
       "2                    Encuentra la capital de España.   \n",
       "3            ¿Cuáles son los tres colores primarios?   \n",
       "4  Genera una lista de 5 adjetivos que describan ...   \n",
       "5                    What is DNA? Answer in english.   \n",
       "6                     What is the capital of France?   \n",
       "7                      What is the capital of Spain?   \n",
       "8                 What are the three primary colors?   \n",
       "9  Generate a list of 5 adjectives that describe ...   \n",
       "\n",
       "                                         Predictions Source  Perplexity  \n",
       "0  El ADN es una molécula de biología molecular q...     ds    1.704543  \n",
       "1                    La capital de Francia es París.     ds    1.064529  \n",
       "2                    La capital de España es Madrid.     ds    1.075414  \n",
       "3  Los tres colores primarios son el rojo, el azu...     ds    1.119825  \n",
       "4     Valiente, corajeoso, audaz, heroico, valiente.     ds    1.775416  \n",
       "5  DNA es una molécula que contiene la informació...  other    1.934921  \n",
       "6                    París es la capital de Francia.  other    1.256139  \n",
       "7                    Madrid es la capital de España.  other    1.162318  \n",
       "8  Los tres colores primarios son el rojo, el azu...  other    1.152615  \n",
       "9       Valiente, coraje, audaz, valiente, valiente.  other    1.732764  "
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "results = pd.DataFrame({\"Prompt\": prompts, \"Predictions\": predictions, \"Source\": sources, \"Perplexity\": perplexities})\n",
    "display(results)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Using model callback to compute perplexity"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_seq(input_ids, eos_token_id, max_len=20, temperature=1):\n",
    "  inputs = input_ids\n",
    "  sequences_stack = []\n",
    "  logit_stack = []\n",
    "\n",
    "  count = 0\n",
    "  while True:\n",
    "    inputs = inputs.to(device)\n",
    "    with torch.no_grad():\n",
    "      output = model(inputs).logits\n",
    "\n",
    "    # Get predicted token: logits > softmax > argmax\n",
    "    logit_stack.append(output[0, -1, :])\n",
    "    probs = F.softmax(output[0, -1, :] / temperature, dim=-1)\n",
    "    sequences_stack.append(torch.argmax(probs, dim=-1))\n",
    "    # Add output to the next input sequence, prompt model autoregressively\n",
    "    inputs = torch.cat((inputs, sequences_stack[-1].reshape(1, 1)), dim=1)\n",
    "\n",
    "    if sequences_stack[-1].item() == eos_token_id or count > max_len:\n",
    "      # Stop generating if the eos token is reached\n",
    "      break\n",
    "\n",
    "    count += 1\n",
    "\n",
    "  logits = torch.stack(logit_stack, dim=0)\n",
    "  logits = logits.reshape(1, logits.shape[0], logits.shape[1])  # Same format as the\n",
    "  sequences = torch.stack(sequences_stack, dim=-1)\n",
    "  sequences = sequences.reshape(1, sequences.shape[0])\n",
    "\n",
    "  return sequences, logits\n",
    "\n",
    "predictions = []\n",
    "perplexities = []\n",
    "temp = 1\n",
    "\n",
    "for i, p in enumerate(prompts):\n",
    "  prompt = \"### Instruction:\\n\"+ p +\"\\n\\n### Response:\\n\"\n",
    "  model_inputs = tokenizer(prompt, return_tensors=\"pt\").to(device)\n",
    "\n",
    "  output_sequences, output_logits = generate_seq(model_inputs.input_ids, tokenizer.eos_token_id, temperature=temp)\n",
    "\n",
    "  loss = F.cross_entropy(input=output_logits[0] / temp, target=output_sequences[0])\n",
    "  ppl = torch.exp(loss)\n",
    "\n",
    "  predictions.append(tokenizer.decode(output_sequences[0], skip_special_tokens=True))\n",
    "  perplexities.append(ppl.item())\n",
    "\n",
    "  # logits = torch.nn.functional.log_softmax(logits, dim=-1) # to get log probabilities from score\n",
    "  # transition_scores = model.compute_transition_scores(output.sequences, output.scores, normalize_logits=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Prompt</th>\n",
       "      <th>Predictions</th>\n",
       "      <th>Source</th>\n",
       "      <th>Perplexity</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>¿Qué significa DNA?</td>\n",
       "      <td>El ADN es una molécula de biología molecular q...</td>\n",
       "      <td>ds</td>\n",
       "      <td>1.735896</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>¿Cuál es la capital de Francia?</td>\n",
       "      <td>La capital de Francia es París.</td>\n",
       "      <td>ds</td>\n",
       "      <td>1.064531</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Encuentra la capital de España.</td>\n",
       "      <td>La capital de España es Madrid.</td>\n",
       "      <td>ds</td>\n",
       "      <td>1.075364</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>¿Cuáles son los tres colores primarios?</td>\n",
       "      <td>Los tres colores primarios son el rojo, el azu...</td>\n",
       "      <td>ds</td>\n",
       "      <td>1.120004</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Genera una lista de 5 adjetivos que describan ...</td>\n",
       "      <td>Valiente, corajeoso, audaz, heroico, valiente.</td>\n",
       "      <td>ds</td>\n",
       "      <td>1.776104</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>What is DNA? Answer in english.</td>\n",
       "      <td>DNA es una molécula que contiene la informació...</td>\n",
       "      <td>other</td>\n",
       "      <td>1.805926</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>What is the capital of France?</td>\n",
       "      <td>París es la capital de Francia.</td>\n",
       "      <td>other</td>\n",
       "      <td>1.257107</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>What is the capital of Spain?</td>\n",
       "      <td>Madrid es la capital de España.</td>\n",
       "      <td>other</td>\n",
       "      <td>1.161920</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>What are the three primary colors?</td>\n",
       "      <td>Los tres colores primarios son el rojo, el azu...</td>\n",
       "      <td>other</td>\n",
       "      <td>1.152752</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>Generate a list of 5 adjectives that describe ...</td>\n",
       "      <td>Valiente, coraje, audaz, valiente, valiente.</td>\n",
       "      <td>other</td>\n",
       "      <td>1.732207</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                              Prompt  \\\n",
       "0                                ¿Qué significa DNA?   \n",
       "1                    ¿Cuál es la capital de Francia?   \n",
       "2                    Encuentra la capital de España.   \n",
       "3            ¿Cuáles son los tres colores primarios?   \n",
       "4  Genera una lista de 5 adjetivos que describan ...   \n",
       "5                    What is DNA? Answer in english.   \n",
       "6                     What is the capital of France?   \n",
       "7                      What is the capital of Spain?   \n",
       "8                 What are the three primary colors?   \n",
       "9  Generate a list of 5 adjectives that describe ...   \n",
       "\n",
       "                                         Predictions Source  Perplexity  \n",
       "0  El ADN es una molécula de biología molecular q...     ds    1.735896  \n",
       "1                    La capital de Francia es París.     ds    1.064531  \n",
       "2                    La capital de España es Madrid.     ds    1.075364  \n",
       "3  Los tres colores primarios son el rojo, el azu...     ds    1.120004  \n",
       "4     Valiente, corajeoso, audaz, heroico, valiente.     ds    1.776104  \n",
       "5  DNA es una molécula que contiene la informació...  other    1.805926  \n",
       "6                    París es la capital de Francia.  other    1.257107  \n",
       "7                    Madrid es la capital de España.  other    1.161920  \n",
       "8  Los tres colores primarios son el rojo, el azu...  other    1.152752  \n",
       "9       Valiente, coraje, audaz, valiente, valiente.  other    1.732207  "
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "results = pd.DataFrame({\"Prompt\": prompts, \"Predictions\": predictions, \"Source\": sources, \"Perplexity\": perplexities})\n",
    "display(results)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Perplexity of the prompts (questions)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "perplexities = []\n",
    "\n",
    "for prompt in prompts:\n",
    "  # prompt = \"### Instruction:\\n\"+ p +\"\\n\\n### Response:\\n\"\n",
    "\n",
    "  ppl = compute_perplexity(prompt, 0)\n",
    "  perplexities.append(ppl.item())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Prompt</th>\n",
       "      <th>Source</th>\n",
       "      <th>Perplexity</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>¿Qué significa DNA?</td>\n",
       "      <td>ds</td>\n",
       "      <td>693.117920</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>¿Cuál es la capital de Francia?</td>\n",
       "      <td>ds</td>\n",
       "      <td>36.610397</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Encuentra la capital de España.</td>\n",
       "      <td>ds</td>\n",
       "      <td>96.876656</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>¿Cuáles son los tres colores primarios?</td>\n",
       "      <td>ds</td>\n",
       "      <td>26.673796</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Genera una lista de 5 adjetivos que describan ...</td>\n",
       "      <td>ds</td>\n",
       "      <td>16.830029</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>What is DNA? Answer in english.</td>\n",
       "      <td>other</td>\n",
       "      <td>356.459351</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>What is the capital of France?</td>\n",
       "      <td>other</td>\n",
       "      <td>133.285233</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>What is the capital of Spain?</td>\n",
       "      <td>other</td>\n",
       "      <td>133.331436</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>What are the three primary colors?</td>\n",
       "      <td>other</td>\n",
       "      <td>234.822952</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>Generate a list of 5 adjectives that describe ...</td>\n",
       "      <td>other</td>\n",
       "      <td>24.460264</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                              Prompt Source  Perplexity\n",
       "0                                ¿Qué significa DNA?     ds  693.117920\n",
       "1                    ¿Cuál es la capital de Francia?     ds   36.610397\n",
       "2                    Encuentra la capital de España.     ds   96.876656\n",
       "3            ¿Cuáles son los tres colores primarios?     ds   26.673796\n",
       "4  Genera una lista de 5 adjetivos que describan ...     ds   16.830029\n",
       "5                    What is DNA? Answer in english.  other  356.459351\n",
       "6                     What is the capital of France?  other  133.285233\n",
       "7                      What is the capital of Spain?  other  133.331436\n",
       "8                 What are the three primary colors?  other  234.822952\n",
       "9  Generate a list of 5 adjectives that describe ...  other   24.460264"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "results = pd.DataFrame({\"Prompt\": prompts, \"Source\": sources, \"Perplexity\": perplexities})\n",
    "display(results)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Issue with logits returned by the generate function\n",
    "Most logits but the max get fixed to -inf, do_sample=False stops that behavior."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/mce/anaconda3/envs/hf/lib/python3.9/site-packages/transformers/generation/configuration_utils.py:381: UserWarning: `do_sample` is set to `False`. However, `temperature` is set to `0.0001` -- this flag is only used in sample-based generation modes. You should set `do_sample=True` or unset `temperature`.\n",
      "  warnings.warn(\n",
      "/home/mce/anaconda3/envs/hf/lib/python3.9/site-packages/transformers/generation/configuration_utils.py:386: UserWarning: `do_sample` is set to `False`. However, `top_p` is set to `0.9` -- this flag is only used in sample-based generation modes. You should set `do_sample=True` or unset `top_p`.\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "predictions = []\n",
    "perplexities = []\n",
    "\n",
    "for i, p in enumerate(prompts):\n",
    "  prompt = \"### Instruction:\\n\"+ p +\"\\n\\n### Response:\\n\"\n",
    "  model_inputs = tokenizer(prompt, return_tensors=\"pt\").to(device)\n",
    "\n",
    "  output = model(model_inputs.input_ids, model_inputs.attention_mask).logits\n",
    "\n",
    "  output_gen = model.generate(**model_inputs, temperature=0.0001, do_sample=False, output_scores=True, return_dict_in_generate=True)\n",
    "\n",
    "  break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Output with model callback (token id):  6489\n",
      "Output with generate (token id):  6489\n",
      "\n",
      "Logits returned by model callback: \n",
      "tensor([ 0.1577,  2.0371,  8.9219,  ..., -0.1796, -0.2822,  2.3770],\n",
      "       device='cuda:0', grad_fn=<SliceBackward0>)\n",
      "Tensor data type:  torch.cuda.FloatTensor\n",
      "Max of the tensor:  tensor(23.8750, device='cuda:0', grad_fn=<MaxBackward1>)\n",
      "\n",
      "Scores returned by model generate: \n",
      "tensor([ 0.1577,  2.0371,  8.9219,  ..., -0.1796, -0.2822,  2.3770],\n",
      "       device='cuda:0')\n",
      "Tensor data type:  torch.cuda.FloatTensor\n",
      "Max of the tensor:  tensor(23.8750, device='cuda:0')\n"
     ]
    }
   ],
   "source": [
    "# Logits of the first output token, not counting the input\n",
    "y = output[0, -1, :] # Logits from the model callback\n",
    "y_gen = output_gen.scores[0][0, :]  # Logits from the generate function\n",
    "\n",
    "# Decoding\n",
    "print(\"Output with model callback (token id): \", y.argmax().item())\n",
    "print(\"Output with generate (token id): \", y_gen.argmax().item())\n",
    "print()\n",
    "\n",
    "# Issue with logits\n",
    "print(\"Logits returned by model callback: \")\n",
    "print(y)\n",
    "print(\"Tensor data type: \", y.type())\n",
    "print(\"Max of the tensor: \", y.max())\n",
    "print()\n",
    "print(\"Scores returned by model generate: \")\n",
    "print(y_gen)\n",
    "print(\"Tensor data type: \", y_gen.type())\n",
    "print(\"Max of the tensor: \", y_gen.max())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Using evaluation package to compute perplexity\n",
    "**Issues:**\n",
    "- It loads the model again, even after setting the environment variable to drive folder where model is cached.\n",
    "- Killed by RAM\n",
    "\n",
    "[Implementation](https://huggingface.co/spaces/evaluate-measurement/perplexity/blob/ac4135177bfee71b1efd7bd3aff62e456e30aef9/perplexity.py)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "# !pip install -q -q evaluate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import evaluate\n",
    "# import os\n",
    "\n",
    "# os.environ['TRANSFORMERS_CACHE'] = '/content/drive/MyDrive/Colab Notebooks/LLMs/cache'\n",
    "# perplexity = evaluate.load(\"perplexity\", module_type=\"metric\")\n",
    "\n",
    "# perplexities = []\n",
    "# for prediction in predictions:\n",
    "#   ppl = perplexity.compute(model_id=model_name, add_start_token=False, predictions=prediction)  # batch_size=1\n",
    "#   print(\"OUTPUT TO THE FUNCTION: \", ppl)\n",
    "#   perplexities.append(ppl)\n",
    "\n",
    "# print(perplexities)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "hf",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
